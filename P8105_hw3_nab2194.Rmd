---
title: "Homework 3"
Author: Natalie Boychuk (nab2194)
output: github_document
---


```{r}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## Problem 1

```{r}
data("instacart") 
```

This dataset contains `r nrow(instacart)` rows (representing the total number of observations) and `r ncol(instacart)` columns. 

Observations are the level of items in order by user. There are user/order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. FINISH UP THIS TEXT 

*How many aisles are there, and from which aisles are the most items ordered?* 

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are 134 aisles in this dataset. Among these, most items are ordered from the 'fresh vegetables,' 'fresh fruits,' and 'packaged vegetables fruits' aisles. Instacart shoppers are a healthy bunch! 

*Making a plot* 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

*Making a table with most popular products in baking ingredients, dog food care, and packaged vegetables/fruit* 
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  knitr:: kable()
```

*Table with mean hour of day at which Pink Lady Apples/Coffee ice cream are ordered* 

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr :: kable()
```

## Problem 2 

```{r bullet point 1 - data tidying}
accel = 
  read_csv("./data/accel_data.csv") 

accel_longer = 
  pivot_longer(
  accel,
  activity.1:activity.1440,
  names_to = "minute_of_day",
  names_prefix = "activity.",
  values_to = "activity_count"
  ) 

accel_wkdy = 
mutate(accel_longer,
       weekday_end = case_when(
         day == "Monday" ~ "weekday",
         day == "Tuesday" ~ "weekday",
         day == "Wednesday" ~ "weekday",
         day == "Thursday" ~ "weekday",
         day == "Friday" ~ "weekday",
         day == "Saturday" ~ "weekend",
         day == "Sunday" ~ "weekend"
         )) %>% 
  mutate(weekday_end = as.factor(weekday_end)) %>% 
  relocate("week","day_id","day","weekday_end", "minute_of_day","activity_count") %>% 
  mutate(minute_of_day = as.double(minute_of_day)) %>% 
  mutate(day = as.factor(day)) %>%
  mutate(
  day = forcats::fct_relevel(day, "Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")) 

```

This dataset provides information on the activity counts for one man over every minute of a 5-week period. After tidying the data, the resulting dataframe has `r nrow(accel_wkdy)` rows and `r ncol(accel_wkdy)` columns. The dataframe includes the week number, an ID for the day number, a binary variable referring to whether the day is a weekday or a weekend (defined as Saturday/Sunday), the minute of the day, and the activity count.

```{r Making a table with aggregate activity}
accel_agg = 
accel_wkdy %>%
  group_by(day_id,day,week) %>% 
  knitr:: kable()
```

*Trends*: 

On first glance, it looks like Fridays tend to be high activity days on average, while Saturdays are lower in activity. It's hard to tell without plotting (part of why plots are important!) but it seems as though Week 3 was a particularly high activity week. 

Creating a plot 
```{r Single-panel plot for activity over every minute}
accel_wkdy %>% 
  ggplot(aes(x = minute_of_day, y = activity_count)) +
  geom_line(aes(), alpha = .2) + 
  geom_smooth(aes(color = day), se = FALSE, alpha = .2) +
  labs(
    title = "Activity Count by Minute for a 5-Week Period",
    x = "Minute of the Day",
    y = "Activity Count",
    caption = "data from accelerometer dataset"
    )
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


## Problem 3 

The NY NOAA dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The dataset covers the minimum and maximum temperature (in tenths of degrees C), precipitation (in tenths of mm), snowfall (mm) and snow depth (mm) in New York state for every day from January 1, 1981 to December 31, 2010. While this dataset is huge, missing data is a pretty significant issue. The maximum/minimum temperature variables are missing 1,134,358 and 1,134,4420 values, respectively. There are also large quantities of missing values for precipitation (145,838), snowfall (381,221), and snow depth (591,786). 

```{r loading in the dataset, tidying, finding the mode}
data("ny_noaa")

clean_nynoaa = 
ny_noaa %>% 
  separate(date,into = c("year","month","day"),convert = "TRUE") %>% 
  mutate(
    tmax = as.double(tmax),
    tmin = as.double(tmin)
  ) %>% 
  mutate(tmax = (tmax/10)) %>% 
  mutate(tmin = (tmin/10))  %>% 
  mutate(prcp = (prcp/10)) 

skimr::skim(clean_nynoaa)

clean_nynoaa %>%
  count(snow) %>% 
  arrange(snow)

clean_nynoaa

```

The most commonly observed value in the snowfall variable is 0, which makes sense since it typically only snows during 1/4 of the year in New York State. It is worth noting that there is one value of -13 for snowfall; this may be an error in the data that should be addressed/better understood before carrying out any analysis. 

```{r two-panel plot of average max temp in Jan/July}
clean_nynoaa %>% 
  mutate(month = as.character(month)) %>% 
  filter(month == c(1,7)) %>% 
  group_by(id,year,month) %>% 
  summarize(
  mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
    geom_point(alpha = .2) + 
    geom_smooth(se = FALSE) + 
    facet_grid(. ~ month) + 
    labs(
    title = "Average Max Temperature in January and July, 1981-2010",
    x = "Year",
    y = "Mean Maximum Temperature",
    caption = "data from NY NOAA"
    )

```




