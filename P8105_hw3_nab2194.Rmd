---
title: "Homework 3"
Author: Natalie Boychuk (nab2194)
output: github_document
---


```{r}
library(tidyverse)
library(p8105.datasets)
library(hexbin)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## Problem 1

```{r}
data("instacart") 
```

This dataset contains `r nrow(instacart)` rows (representing the total number of observations) and `r ncol(instacart)` columns. 

Observations are the level of items in order by user. There are user/order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. FINISH UP THIS TEXT 

*How many aisles are there, and from which aisles are the most items ordered?* 

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are 134 aisles in this dataset. Among these, most items are ordered from the 'fresh vegetables,' 'fresh fruits,' and 'packaged vegetables fruits' aisles. Instacart shoppers are a healthy bunch! 

*Making a plot* 

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

*Making a table with most popular products in baking ingredients, dog food care, and packaged vegetables/fruit* 
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  knitr:: kable()
```

*Table with mean hour of day at which Pink Lady Apples/Coffee ice cream are ordered* 

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr :: kable()
```

## Problem 2 

```{r bullet point 1 - data tidying}
accel = 
  read_csv("./data/accel_data.csv") 

accel_longer = 
  pivot_longer(
  accel,
  activity.1:activity.1440,
  names_to = "minute_of_day",
  names_prefix = "activity.",
  values_to = "activity_count"
  ) 

accel_wkdy = 
mutate(accel_longer,
       weekday_end = case_when(
         day == "Monday" ~ "weekday",
         day == "Tuesday" ~ "weekday",
         day == "Wednesday" ~ "weekday",
         day == "Thursday" ~ "weekday",
         day == "Friday" ~ "weekday",
         day == "Saturday" ~ "weekend",
         day == "Sunday" ~ "weekend"
         )) %>% 
  mutate(weekday_end = as.factor(weekday_end)) %>% 
  relocate("week","day_id","day","weekday_end", "minute_of_day","activity_count") %>% 
  mutate(minute_of_day = as.double(minute_of_day)) %>% 
  mutate(day = as.factor(day)) %>%
  mutate(
  day = forcats::fct_relevel(day, "Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))
  
```

This dataset provides information on the activity counts for one man over every minute of a 5-week period. After tidying the data, the resulting dataframe has `r nrow(accel_wkdy)` rows and `r ncol(accel_wkdy)` columns. The dataframe includes the week number, an ID for the day number, a binary variable referring to whether the day is a weekday or a weekend (defined as Saturday/Sunday), the minute of the day, and the activity count.

```{r Making a table with aggregate activity}
accel_agg = 
accel_wkdy %>%
   group_by(day_id,day,week) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
   mutate(day = forcats::fct_relevel(day,"Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))

knitr::kable(accel_agg)

```

*Trends*: 

On first glance, it looks like Fridays tend to be high activity days on average, while Saturdays are lower in activity. It's hard to tell without plotting (part of why plots are important!) but it seems as though Week 3 was a particularly high activity week. 

Creating a plot 
```{r Single-panel plot for activity over every minute}
accel_wkdy %>% 
  ggplot(aes(x = minute_of_day, y = activity_count)) +
  geom_line(aes(color = day), se = FALSE, alpha = .5) +
  labs(
    title = "Activity Count by Minute for a 5-Week Period",
    x = "Minute of the Day",
    y = "Activity Count",
    caption = "data from accelerometer dataset"
    ) 

```

There is generally low activity from minutes 0-500, which makes sense, since the participant is probably sleeping. There is a significant spike at minute 1000 and around 1250, which corresponds to about 4:30PM and 6:30PM. This also makes sense, since this could be a high-activity time for socializing and doing activities, particularly on Friday and Saturday when we see spikes. 

## Problem 3 

The NY NOAA dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The dataset covers the minimum and maximum temperature (in tenths of degrees C), precipitation (in tenths of mm), snowfall (mm) and snow depth (mm) in New York state for every day from January 1, 1981 to December 31, 2010. While this dataset is huge, missing data is a pretty significant issue. The maximum/minimum temperature variables are missing 1,134,358 and 1,134,4420 values, respectively. There are also large quantities of missing values for precipitation (145,838), snowfall (381,221), and snow depth (591,786). 

```{r loading in the dataset, tidying, finding the mode}
data("ny_noaa")

clean_nynoaa = 
ny_noaa %>% 
  separate(date,into = c("year","month","day"),convert = "TRUE") %>% 
  mutate(
    tmax = as.double(tmax),
    tmin = as.double(tmin)
  ) %>% 
  mutate(tmax = (tmax/10)) %>% 
  mutate(tmin = (tmin/10)) %>% 
  mutate(prcp = (prcp/10)) 

clean_nynoaa %>%
  count(snow) %>% 
  arrange(snow)

```

The most commonly observed value in the snowfall variable is 0, which makes sense since it typically only snows during 1/4 of the year in New York State. It is worth noting that there is one value of -13 for snowfall; this may be an error in the data that should be addressed/better understood before carrying out any analysis. 

```{r two-panel plot of average max temp in Jan/July}
clean_nynoaa %>% 
  mutate(month = as.character(month)) %>% 
  filter(month == c(1,7)) %>% 
  group_by(id,year,month) %>% 
  summarize(
  mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
    geom_point(alpha = .3) +
    geom_smooth(se = FALSE) +
    facet_grid(. ~ month) + 
    labs(
    title = "Average Max Temperature in January and July, 1981-2010",
    x = "Year",
    y = "Mean Maximum Temperature",
    caption = "data from NY NOAA"
    )
```

*Trends*: 
The average maximum temperature is obviously much warmer in July (hovering around 30 degrees Celsius) compared to January (ranging from around -5 degrees Celsius to a little under 5 degrees Celsius). There were higher average maximum temperatures in January in 1990, 2000, and around 2008, and this relationship appears to be relatively cyclical. There are a number of outliers in the July plot - in 1984, 1988, 2004, and around 2006, there were unseasonably cold July days (from just under 15 degrees Celsius to just over 20 degrees Celsius). 

```{r Second plot with tmax vs. tmin for the full dataset and patchwork for snow distribution }

tminmax = (clean_nynoaa %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(
    x = "Maximum Temperature",
    y = "Minimum Temperature",
    caption = "data from NY NOAA"))

library(patchwork)
snowdis = (clean_nynoaa %>% 
     filter(snow > 0, snow < 100) %>% 
     group_by(year) %>% 
     ggplot(aes(x = snow, y = snow)) + 
     geom_boxplot() + 
     facet_wrap(.~ year))  

tminmax + snowdis
```

*Comments*: 
This plot shows a comparison between maximum and minimum temperatures on the left and the average distribution of snowfall for each year (1981-2010) on the left. The Hexplot shows that the minimum and maximum temperatures tend to overlap at around a minimum temperature of 15 degrees Celsius and a maximum temperature of around 20 degrees Celsius.

As you can see, there were outliers in the amount of snowfall in 1998, 2006, and 2010, with all of these years experiencing single days of 75mm or more snow. It seems as though most years had relatively similar snowfall, other than those three years. 


