Homework 3
================

``` r
library(tidyverse)
```

    ## ── Attaching packages ───────────────────────────────────── tidyverse 1.3.0 ──

    ## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
    ## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
    ## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
    ## ✓ readr   1.3.1     ✓ forcats 0.5.0

    ## ── Conflicts ──────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(hexbin)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

``` r
data("instacart") 
```

This dataset contains 1384617 rows (representing the total number of
observations) and 15 columns.

Observations are the level of items in order by user. There are
user/order variables – user ID, order ID, order day, and order hour.
There are also item variables – name, aisle, department, and some
numeric codes. FINISH UP THIS TEXT

*How many aisles are there, and from which aisles are the most items
ordered?*

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

There are 134 aisles in this dataset. Among these, most items are
ordered from the ‘fresh vegetables,’ ‘fresh fruits,’ and ‘packaged
vegetables fruits’ aisles. Instacart shoppers are a healthy bunch\!

*Making a plot*

``` r
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="P8105_hw3_nab2194_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

*Making a table with most popular products in baking ingredients, dog
food care, and packaged vegetables/fruit*

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  knitr:: kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |

*Table with mean hour of day at which Pink Lady Apples/Coffee ice cream
are ordered*

``` r
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr :: kable()
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

| product\_name    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |
| :--------------- | -------: | -------: | -------: | -------: | -------: | -------: | -------: |
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 | 15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 | 14.25000 | 11.55172 | 12.78431 | 11.93750 |

## Problem 2

``` r
accel = 
  read_csv("./data/accel_data.csv") 
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
accel_longer = 
  pivot_longer(
  accel,
  activity.1:activity.1440,
  names_to = "minute_of_day",
  names_prefix = "activity.",
  values_to = "activity_count"
  ) 

accel_wkdy = 
mutate(accel_longer,
       weekday_end = case_when(
         day == "Monday" ~ "weekday",
         day == "Tuesday" ~ "weekday",
         day == "Wednesday" ~ "weekday",
         day == "Thursday" ~ "weekday",
         day == "Friday" ~ "weekday",
         day == "Saturday" ~ "weekend",
         day == "Sunday" ~ "weekend"
         )) %>% 
  mutate(weekday_end = as.factor(weekday_end)) %>% 
  relocate("week","day_id","day","weekday_end", "minute_of_day","activity_count") %>% 
  mutate(minute_of_day = as.double(minute_of_day)) %>% 
  mutate(day = as.factor(day)) %>%
  mutate(
  day = forcats::fct_relevel(day, "Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))
```

This dataset provides information on the activity counts for one man
over every minute of a 5-week period. After tidying the data, the
resulting dataframe has 50400 rows and 6 columns. The dataframe includes
the week number, an ID for the day number, a binary variable referring
to whether the day is a weekday or a weekend (defined as
Saturday/Sunday), the minute of the day, and the activity count.

``` r
accel_agg = 
accel_wkdy %>%
   group_by(day_id,day,week) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
   mutate(day = forcats::fct_relevel(day,"Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
```

    ## `summarise()` regrouping output by 'day_id', 'day' (override with `.groups` argument)

``` r
knitr::kable(accel_agg)
```

| day\_id | day       | week | total\_activity |
| ------: | :-------- | ---: | --------------: |
|       1 | Friday    |    1 |       480542.62 |
|       2 | Monday    |    1 |        78828.07 |
|       3 | Saturday  |    1 |       376254.00 |
|       4 | Sunday    |    1 |       631105.00 |
|       5 | Thursday  |    1 |       355923.64 |
|       6 | Tuesday   |    1 |       307094.24 |
|       7 | Wednesday |    1 |       340115.01 |
|       8 | Friday    |    2 |       568839.00 |
|       9 | Monday    |    2 |       295431.00 |
|      10 | Saturday  |    2 |       607175.00 |
|      11 | Sunday    |    2 |       422018.00 |
|      12 | Thursday  |    2 |       474048.00 |
|      13 | Tuesday   |    2 |       423245.00 |
|      14 | Wednesday |    2 |       440962.00 |
|      15 | Friday    |    3 |       467420.00 |
|      16 | Monday    |    3 |       685910.00 |
|      17 | Saturday  |    3 |       382928.00 |
|      18 | Sunday    |    3 |       467052.00 |
|      19 | Thursday  |    3 |       371230.00 |
|      20 | Tuesday   |    3 |       381507.00 |
|      21 | Wednesday |    3 |       468869.00 |
|      22 | Friday    |    4 |       154049.00 |
|      23 | Monday    |    4 |       409450.00 |
|      24 | Saturday  |    4 |         1440.00 |
|      25 | Sunday    |    4 |       260617.00 |
|      26 | Thursday  |    4 |       340291.00 |
|      27 | Tuesday   |    4 |       319568.00 |
|      28 | Wednesday |    4 |       434460.00 |
|      29 | Friday    |    5 |       620860.00 |
|      30 | Monday    |    5 |       389080.00 |
|      31 | Saturday  |    5 |         1440.00 |
|      32 | Sunday    |    5 |       138421.00 |
|      33 | Thursday  |    5 |       549658.00 |
|      34 | Tuesday   |    5 |       367824.00 |
|      35 | Wednesday |    5 |       445366.00 |

*Trends*:

On first glance, it looks like Fridays tend to be high activity days on
average, while Saturdays are lower in activity. It’s hard to tell
without plotting (part of why plots are important\!) but it seems as
though Week 3 was a particularly high activity week.

Creating a plot

``` r
accel_wkdy %>% 
  ggplot(aes(x = minute_of_day, y = activity_count)) +
  geom_line(aes(color = day), se = FALSE, alpha = .5) +
  labs(
    title = "Activity Count by Minute for a 5-Week Period",
    x = "Minute of the Day",
    y = "Activity Count",
    caption = "data from accelerometer dataset"
    ) 
```

    ## Warning: Ignoring unknown parameters: se

<img src="P8105_hw3_nab2194_files/figure-gfm/Single-panel plot for activity over every minute-1.png" width="90%" />

There is generally low activity from minutes 0-500, which makes sense,
since the participant is probably sleeping. There is a significant spike
at minute 1000 and around 1250, which corresponds to about 4:30PM and
6:30PM. This also makes sense, since this could be a high-activity time
for socializing and doing activities, particularly on Friday and
Saturday when we see spikes.

## Problem 3

The NY NOAA dataset has 2595176 rows and 7 columns. The dataset covers
the minimum and maximum temperature (in tenths of degrees C),
precipitation (in tenths of mm), snowfall (mm) and snow depth (mm) in
New York state for every day from January 1, 1981 to December 31, 2010.
While this dataset is huge, missing data is a pretty significant issue.
The maximum/minimum temperature variables are missing 1,134,358 and
1,134,4420 values, respectively. There are also large quantities of
missing values for precipitation (145,838), snowfall (381,221), and snow
depth (591,786).

``` r
data("ny_noaa")

clean_nynoaa = 
ny_noaa %>% 
  separate(date,into = c("year","month","day"),convert = "TRUE") %>% 
  mutate(
    tmax = as.double(tmax),
    tmin = as.double(tmin)
  ) %>% 
  mutate(tmax = (tmax/10)) %>% 
  mutate(tmin = (tmin/10)) %>% 
  mutate(prcp = (prcp/10)) 

clean_nynoaa %>%
  count(snow) %>% 
  arrange(snow)
```

    ## # A tibble: 282 x 2
    ##     snow       n
    ##    <int>   <int>
    ##  1   -13       1
    ##  2     0 2008508
    ##  3     3    8790
    ##  4     5    9748
    ##  5     8    9962
    ##  6    10    5106
    ##  7    13   23095
    ##  8    15    3672
    ##  9    18    3226
    ## 10    20    4797
    ## # … with 272 more rows

The most commonly observed value in the snowfall variable is 0, which
makes sense since it typically only snows during 1/4 of the year in New
York State. It is worth noting that there is one value of -13 for
snowfall; this may be an error in the data that should be
addressed/better understood before carrying out any analysis.

``` r
clean_nynoaa %>% 
  mutate(month = as.character(month)) %>% 
  filter(month == c(1,7)) %>% 
  group_by(id,year,month) %>% 
  summarize(
  mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + 
    geom_point(alpha = .3) +
    geom_smooth(se = FALSE) +
    facet_grid(. ~ month) + 
    labs(
    title = "Average Max Temperature in January and July, 1981-2010",
    x = "Year",
    y = "Mean Maximum Temperature",
    caption = "data from NY NOAA"
    )
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

    ## Warning: Removed 6843 rows containing non-finite values (stat_smooth).

    ## Warning: Removed 6843 rows containing missing values (geom_point).

<img src="P8105_hw3_nab2194_files/figure-gfm/two-panel plot of average max temp in Jan/July-1.png" width="90%" />

*Trends*: The average maximum temperature is obviously much warmer in
July (hovering around 30 degrees Celsius) compared to January (ranging
from around -5 degrees Celsius to a little under 5 degrees Celsius).
There were higher average maximum temperatures in January in 1990, 2000,
and around 2008, and this relationship appears to be relatively
cyclical. There are a number of outliers in the July plot - in 1984,
1988, 2004, and around 2006, there were unseasonably cold July days
(from just under 15 degrees Celsius to just over 20 degrees Celsius).

``` r
tminmax = (clean_nynoaa %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(
    x = "Maximum Temperature",
    y = "Minimum Temperature",
    caption = "data from NY NOAA"))

library(patchwork)
snowdis = (clean_nynoaa %>% 
     filter(snow > 0, snow < 100) %>% 
     group_by(year) %>% 
     ggplot(aes(x = snow, y = snow)) + 
     geom_boxplot() + 
     facet_wrap(.~ year))  

tminmax + snowdis
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

    ## Warning: Continuous x aesthetic -- did you forget aes(group=...)?

<img src="P8105_hw3_nab2194_files/figure-gfm/Second plot with tmax vs. tmin for the full dataset and patchwork for snow distribution-1.png" width="90%" />

*Comments*: This plot shows a comparison between maximum and minimum
temperatures on the left and the average distribution of snowfall for
each year (1981-2010) on the left. The Hexplot shows that the minimum
and maximum temperatures tend to overlap at around a minimum temperature
of 15 degrees Celsius and a maximum temperature of around 20 degrees
Celsius.

As you can see, there were outliers in the amount of snowfall in 1998,
2006, and 2010, with all of these years experiencing single days of 75mm
or more snow. It seems as though most years had relatively similar
snowfall, other than those three years.
